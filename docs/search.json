[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html",
    "title": "Bank Marketing Dataset",
    "section": "",
    "text": "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#data-set-information",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#data-set-information",
    "title": "Bank Marketing Dataset",
    "section": "Data Set Information:",
    "text": "Data Set Information:\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (‘yes’) or not (‘no’) subscribed.\nThere are four datasets: - bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014] - bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs. - bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). - bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs). The smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM).\nThe classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y)."
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#crisp-dm-framework",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#crisp-dm-framework",
    "title": "Bank Marketing Dataset",
    "section": "CRISP-DM Framework",
    "text": "CRISP-DM Framework\nCRISP-DM is a standard process for data science projects in the industry. I will be framing my analysis on this datset referring to this methodology.\n\nBusiness Understanding\nThe primary goal of using this dataset is to compare the performance of various classifers for this classification problem.\n\n\nAttribute Information:\nInput variables: #### bank client data: 1. age (numeric) 2. job : type of job (categorical: ‘admin.’,‘blue-collar’,‘entrepreneur’,‘housemaid’,‘management’,‘retired’,‘self-employed’,‘services’,‘student’,‘technician’,‘unemployed’,‘unknown’) 3. marital : marital status (categorical: ‘divorced’,‘married’,‘single’,‘unknown’; note: ‘divorced’ means divorced or widowed) 4. education (categorical: ‘basic.4y’,‘basic.6y’,‘basic.9y’,‘high.school’,‘illiterate’,‘professional.course’,‘university.degree’,‘unknown’) 5. default: has credit in default? (categorical: ‘no’,‘yes’,‘unknown’) 6. housing: has housing loan? (categorical: ‘no’,‘yes’,‘unknown’) 7. loan: has personal loan? (categorical: ‘no’,‘yes’,‘unknown’) #### related with the last contact of the current campaign: 8. contact: contact communication type (categorical: ‘cellular’,‘telephone’) 9. month: last contact month of year (categorical: ‘jan’, ‘feb’, ‘mar’, …, ‘nov’, ‘dec’) 10. day_of_week: last contact day of the week (categorical: ‘mon’,‘tue’,‘wed’,‘thu’,‘fri’) 11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. #### other attributes: 12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact) 13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted) 14. previous: number of contacts performed before this campaign and for this client (numeric) 15. poutcome: outcome of the previous marketing campaign (categorical: ‘failure’,‘nonexistent’,‘success’) #### social and economic context attributes 16. emp.var.rate: employment variation rate - quarterly indicator (numeric) 17. cons.price.idx: consumer price index - monthly indicator (numeric) 18. cons.conf.idx: consumer confidence index - monthly indicator (numeric) 19. euribor3m: euribor 3 month rate - daily indicator (numeric) 20. nr.employed: number of employees - quarterly indicator (numeric)\nOutput variable (desired target): 21. y - has the client subscribed a term deposit? (binary: ‘yes’,‘no’)"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#data-understanding",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#data-understanding",
    "title": "Bank Marketing Dataset",
    "section": "Data Understanding",
    "text": "Data Understanding\n\n#imports\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom time import time\n\n#visualization params\nsns.set()\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n#read the data\ndf = pd.read_csv('data/bank-additional-full.csv',sep=';')\n\n#head\ndf.head()\n\n\n\n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nhousing\nloan\ncontact\nmonth\nday_of_week\n...\ncampaign\npdays\nprevious\npoutcome\nemp.var.rate\ncons.price.idx\ncons.conf.idx\neuribor3m\nnr.employed\ny\n\n\n\n\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n#info\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 41188 entries, 0 to 41187\nData columns (total 21 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   age             41188 non-null  int64  \n 1   job             41188 non-null  object \n 2   marital         41188 non-null  object \n 3   education       41188 non-null  object \n 4   default         41188 non-null  object \n 5   housing         41188 non-null  object \n 6   loan            41188 non-null  object \n 7   contact         41188 non-null  object \n 8   month           41188 non-null  object \n 9   day_of_week     41188 non-null  object \n 10  duration        41188 non-null  int64  \n 11  campaign        41188 non-null  int64  \n 12  pdays           41188 non-null  int64  \n 13  previous        41188 non-null  int64  \n 14  poutcome        41188 non-null  object \n 15  emp.var.rate    41188 non-null  float64\n 16  cons.price.idx  41188 non-null  float64\n 17  cons.conf.idx   41188 non-null  float64\n 18  euribor3m       41188 non-null  float64\n 19  nr.employed     41188 non-null  float64\n 20  y               41188 non-null  object \ndtypes: float64(5), int64(5), object(11)\nmemory usage: 6.6+ MB\n\n\n\n#check for missing values\ndf.isna().sum()\n\nage               0\njob               0\nmarital           0\neducation         0\ndefault           0\nhousing           0\nloan              0\ncontact           0\nmonth             0\nday_of_week       0\nduration          0\ncampaign          0\npdays             0\nprevious          0\npoutcome          0\nemp.var.rate      0\ncons.price.idx    0\ncons.conf.idx     0\neuribor3m         0\nnr.employed       0\ny                 0\ndtype: int64\n\n\nThere are no missing values in provided dataset.\n\n#pairplot of numerical features \n\nsns.pairplot(df,hue='y')\nplt.title('Pairplot of Numerical features');\n\n\n\n\n\n\n\n\n\n#age\nsns.displot(data=df,x='age',hue='y',kind='hist',palette='viridis',multiple='stack')\nplt.title('Age distribution')\nplt.figure(figsize=(12,8))\ndf.age.dtype\n\ndtype('int64')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#job\nsns.displot(data=df,x='job',kind='hist',palette='viridis',hue='y',multiple='stack')\nplt.xticks(rotation=45)\nplt.title('Job distribution')\nplt.figure(figsize=(12,8))\ndf.job.dtype\n\ndtype('O')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#marital\nsns.displot(data=df,x='marital',kind='hist',palette='viridis',hue='y',multiple='stack')\nplt.xticks(rotation=45)\nplt.title('Marital Status Distribution')\nplt.figure(figsize=(12,8))\ndf.marital.dtype\n\ndtype('O')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#Education\nsns.displot(data=df,x='education',kind='hist',palette='viridis',hue='y',multiple='stack')\nplt.xticks(rotation=45)\nplt.title('Education Distribution')\nplt.figure(figsize=(12,8))\ndf.education.dtype\n\ndtype('O')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#default\nsns.displot(data=df,x='default',kind='hist',palette='viridis',hue='y',multiple='stack')\nplt.xticks(rotation=45)\nplt.title('Default Distribution')\nplt.figure(figsize=(12,8))\ndf.default.dtype\n\ndtype('O')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#housing\nsns.displot(data=df,x='housing',kind='hist',palette='viridis',hue='y',multiple='stack')\nplt.xticks(rotation=45)\nplt.title('Housing Status Distribution')\nplt.figure(figsize=(12,8))\ndf.housing.dtype\n\ndtype('O')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#loan\nsns.displot(data=df,x='loan',kind='hist',palette='viridis',hue='y',multiple='stack')\nplt.xticks(rotation=45)\nplt.title('Loan Status Distribution')\nplt.figure(figsize=(12,8))\ndf.loan.dtype\n\ndtype('O')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#contact\nsns.displot(data=df,x='contact',kind='hist',palette='viridis',hue='y',multiple='stack')\nplt.xticks(rotation=45)\nplt.title('Contact Distribution')\nplt.figure(figsize=(12,8))\ndf.contact.dtype\n\ndtype('O')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#month\nsns.displot(data=df,x='month',kind='hist',palette='viridis',hue='y',multiple='stack')\nplt.xticks(rotation=45)\nplt.title('Month Distribution')\nplt.figure(figsize=(12,8))\ndf.month.dtype\n\ndtype('O')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#day of the week\nsns.displot(df,x='day_of_week',kind='hist',hue='y',multiple='stack',palette='viridis')\nplt.xticks(rotation=45)\nplt.figure(figsize=(12,8))\ndf['day_of_week'].dtype\n\ndtype('O')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#duration\nsns.displot(data=df,x='duration',hue='y',kind='hist',palette='viridis',multiple='stack')\nplt.title('Duration distribution')\nplt.figure(figsize=(12,8))\ndf.duration.dtype\n\ndtype('int64')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#employment variation rate\n\nsns.displot(data=df,x='emp.var.rate',hue='y',kind='hist',palette='viridis',multiple='stack')\nplt.title('Employment Variation Rate Distribution')\nplt.figure(figsize=(12,8))\ndf['emp.var.rate'].dtype\n\ndtype('float64')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#cons.price.idx\nsns.displot(data=df,x='cons.price.idx',hue='y',kind='hist',palette='viridis',multiple='stack')\nplt.title('Consumer Price Index distribution')\nplt.figure(figsize=(12,8))\ndf['cons.price.idx'].dtype\n\ndtype('float64')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#euribor3m\nsns.displot(data=df,x='euribor3m',hue='y',kind='hist',palette='viridis',multiple='stack')\nplt.title('Euribor 3 month distribution')\nplt.figure(figsize=(12,8))\ndf.euribor3m.dtype\n\ndtype('float64')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;\n\n\n\n#number of employees - quarterly indicator\nsns.displot(data=df,x='nr.employed',hue='y',kind='hist',palette='viridis',multiple='stack')\nplt.title('Number of employees - Quarterly distribution')\nplt.figure(figsize=(12,8))\ndf['nr.employed'].dtype\n\ndtype('float64')\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1200x800 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#modeling",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#modeling",
    "title": "Bank Marketing Dataset",
    "section": "Modeling",
    "text": "Modeling\nSince this is classification problem, I will being using various classification algorithms provided out of box using Sci-kit learn module.\n\n#imports\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_selector,ColumnTransformer\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n\n#Test Train Split\nX = df.drop(['campaign','pdays','previous','poutcome','y'],axis=1)\ny = df.y\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(X.shape)\nprint(y.shape)\n\n(41188, 16)\n(41188,)"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#preprocessing",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#preprocessing",
    "title": "Bank Marketing Dataset",
    "section": "Preprocessing",
    "text": "Preprocessing\nGeneral preprocessing steps that are common for all the classifiers. The categorical features are one hot encoded and the numerical features are scaled using a StandardScaler algorithm.\n\n#Pre-processing\n\n#selecting the numerical columns\nnum_cols = make_column_selector(dtype_include=('int64','float64'))\n\n#Preprocessing\npre = ColumnTransformer([\n    ('scaler',StandardScaler(),num_cols)\n],remainder = OneHotEncoder())\n\nAll the classifiers are built using a pipeline, which takes the input from the column transformer which contains our pre-processing steps."
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#logistic-regression",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#logistic-regression",
    "title": "Bank Marketing Dataset",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n#Logistic Regression pipeline\npipe_lr = Pipeline([\n    ('pre',pre),\n    ('lr',LogisticRegression(max_iter=1000,random_state=42))\n])\npipe_lr\n\nPipeline(steps=[('pre',\n                 ColumnTransformer(remainder=OneHotEncoder(),\n                                   transformers=[('scaler', StandardScaler(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x168515e40&gt;)])),\n                ('lr', LogisticRegression(max_iter=1000, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('pre',\n                 ColumnTransformer(remainder=OneHotEncoder(),\n                                   transformers=[('scaler', StandardScaler(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x168515e40&gt;)])),\n                ('lr', LogisticRegression(max_iter=1000, random_state=42))])pre: ColumnTransformerColumnTransformer(remainder=OneHotEncoder(),\n                  transformers=[('scaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x168515e40&gt;)])scaler&lt;sklearn.compose._column_transformer.make_column_selector object at 0x168515e40&gt;StandardScalerStandardScaler()remainderOneHotEncoderOneHotEncoder()LogisticRegressionLogisticRegression(max_iter=1000, random_state=42)\n\n\n\n#Model Training\npipe_lr.fit(X_train,y_train)\n\n#Model prediction\ny_pred = pipe_lr.predict(X_test)\ny_pred_train = pipe_lr.predict(X_train)\n\n#Model Evaluation\ntrain_score = accuracy_score(y_train,y_pred_train)\nprint('Train Accuracy:',round(train_score*100,4))\ntest_acc_score = accuracy_score(y_test,y_pred)\nprint('Test Accuracy:',round(test_acc_score*100,4))\ntest_f1_score = f1_score(y_test,y_pred,pos_label='yes',average='binary')\nprint('Test F1 score:',round(test_f1_score*100,4))\n\n#Classification Report\nprint(classification_report(y_test,y_pred))\n\n#Confusion Matrix Heatmap\nplt.title('Logistic Regression')\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\");\n\nTrain Accuracy: 90.5554\nTest Accuracy: 90.6531\nTest F1 score: 48.0432\n              precision    recall  f1-score   support\n\n          no       0.92      0.97      0.95      7303\n         yes       0.65      0.38      0.48       935\n\n    accuracy                           0.91      8238\n   macro avg       0.79      0.68      0.71      8238\nweighted avg       0.89      0.91      0.90      8238\n\n\n\n\n\n\n\n\n\n\n\n#GridSearch for Optimal Hyperparameters\n\nparam_grid = {\n    'lr__C':(0.01,0.1,1),\n    'lr__solver':('newton-cg', 'lbfgs','liblinear','sag','saga'),\n#    'lr__class_weight':(None,'balanced')\n}\n\ngrid_lr = GridSearchCV(pipe_lr,param_grid,scoring='accuracy',verbose=1)\nstart = time()\ngrid_lr.fit(X_train,y_train)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(grid_lr.cv_results_[\"params\"]))\n)\n\n#best paramater available in the grid\nprint('The best params are\\n',grid_lr.best_params_)\n\n#Refit Model Predictions\ny_pred = grid_lr.predict(X_test)\ny_pred_train = grid_lr.predict(X_train)\n\n#Refit Model Evaluation\ntrain_score = accuracy_score(y_train,y_pred_train)\nprint('Train Accuracy:',round(train_score*100,4))\ntest_acc_score = accuracy_score(y_test,y_pred)\nprint('Test Accuracy:',round(test_acc_score*100,4))\ntest_f1_score = f1_score(y_test,y_pred,pos_label='yes',average='binary')\nprint('Test F1 score:',round(test_f1_score*100,4))\n\n#Refit model Classification report\nprint(classification_report(y_test,y_pred))\n\n#Confusion Matrix Heatmap\nplt.title('Logistic Regression Classifier')\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\");\n\nFitting 5 folds for each of 15 candidates, totalling 75 fits\nGridSearchCV took 54.03 seconds for 15 candidate parameter settings.\nThe best params are\n {'lr__C': 0.01, 'lr__solver': 'sag'}\nTrain Accuracy: 90.5888\nTest Accuracy: 90.6531\nTest F1 score: 46.3788\n              precision    recall  f1-score   support\n\n          no       0.92      0.98      0.95      7303\n         yes       0.66      0.36      0.46       935\n\n    accuracy                           0.91      8238\n   macro avg       0.79      0.67      0.71      8238\nweighted avg       0.89      0.91      0.89      8238\n\n\n\n\n\n\n\n\n\n\n\n#Appending the results to a results\n\nresults_lr = {\n    'Model':['Logistic Regression'],\n    'Train Score':[round(train_score*100,4)],\n    'Test Score':[round(test_acc_score*100,4)],\n    'Refit Time':[round(grid_lr.refit_time_,4)]\n}\nresults = pd.DataFrame.from_dict(results_lr)\nresults\n\n\n\n\n\n\n\n\n\nModel\nTrain Score\nTest Score\nRefit Time\n\n\n\n\n0\nLogistic Regression\n90.5888\n90.6531\n2.2281"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#why-refit-time",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#why-refit-time",
    "title": "Bank Marketing Dataset",
    "section": "Why Refit Time?",
    "text": "Why Refit Time?\nRefit time is the amount of the time taken to fit the model using the best parameters found throught the grid search. This helps us understand how long it would have taken to train the model. This metric can be used to compare the training time amongst various models."
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#k-nearest-neighbors",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#k-nearest-neighbors",
    "title": "Bank Marketing Dataset",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\n\n#KNN pipeline\npipe_knn = Pipeline([\n    ('pre',pre),\n    ('knn',KNeighborsClassifier())\n])\n\n#Model Training and predictions\npipe_knn.fit(X_train,y_train)\ny_pred = pipe_knn.predict(X_test)\ny_pred_train = pipe_knn.predict(X_train)\n\n#Model Evaluation\ntrain_score = accuracy_score(y_train,y_pred_train)\nprint('Train Accuracy:',round(train_score*100,4))\ntest_acc_score = accuracy_score(y_test,y_pred)\nprint('Test Accuracy:',round(test_acc_score*100,4))\ntest_f1_score = f1_score(y_test,y_pred,pos_label='yes',average='binary')\nprint('Test F1 score:',round(test_f1_score*100,4))\n\n#Classification report\nprint(classification_report(y_test,y_pred))\n\n#Confusion Matrix Heatmap\nplt.title('K-Nearest Neighbors Classifier')\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\");\n\nTrain Accuracy: 92.7344\nTest Accuracy: 89.8034\nTest F1 score: 49.1525\n              precision    recall  f1-score   support\n\n          no       0.93      0.96      0.94      7303\n         yes       0.57      0.43      0.49       935\n\n    accuracy                           0.90      8238\n   macro avg       0.75      0.70      0.72      8238\nweighted avg       0.89      0.90      0.89      8238\n\n\n\n\n\n\n\n\n\n\n\n#Gridsearch and refitting\n\nparam_grid = {\n    'knn__n_neighbors': range(2,5,1),\n#    'knn__weights':('uniform','distance'),\n#    'knn__algorithm':('auto','ball_tree','kd_tree','brute')\n}\n\ngrid_knn = GridSearchCV(pipe_knn,param_grid,scoring='accuracy',verbose=1)\nstart = time()\ngrid_knn.fit(X_train, y_train)\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"% (time() - start, len(grid_knn.cv_results_[\"params\"])))\n#best params\nprint('The best params are\\n',grid_knn.best_params_)\n\n#Refit Model Predictions\ny_pred = grid_knn.predict(X_test)\ny_pred_train = grid_knn.predict(X_train)\n\n#Refit Model Evaluation\ntrain_score = accuracy_score(y_train,y_pred_train)\nprint('Train Accuracy:',round(train_score*100,4))\ntest_acc_score = accuracy_score(y_test,y_pred)\nprint('Test Accuracy:',round(test_acc_score*100,4))\ntest_f1_score = f1_score(y_test,y_pred,pos_label='yes',average='binary')\nprint('Test F1 score:',round(test_f1_score*100,4))\n\n#Refit model Classification report\nprint(classification_report(y_test,y_pred))\n\n#Confusion Matrix Heatmap\nplt.title('K-Nearest Neighbors Classifier')\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\");\n\nFitting 5 folds for each of 3 candidates, totalling 15 fits\nGridSearchCV took 80.70 seconds for 3 candidate parameter settings.\nThe best params are\n {'knn__n_neighbors': 4}\nTrain Accuracy: 92.6191\nTest Accuracy: 89.4877\nTest F1 score: 37.4277\n              precision    recall  f1-score   support\n\n          no       0.91      0.97      0.94      7303\n         yes       0.58      0.28      0.37       935\n\n    accuracy                           0.89      8238\n   macro avg       0.75      0.63      0.66      8238\nweighted avg       0.88      0.89      0.88      8238\n\n\n\n\n\n\n\n\n\n\n\n#appending the results Dataframe\n\nresults_knn = {\n    'Model':['K-Nearest Neighbors'],\n    'Train Score':[round(train_score*100,4)],\n    'Test Score':[round(test_acc_score*100,4)],\n    'Refit Time':[round(grid_knn.refit_time_,4)]\n}\nresults_knn = pd.DataFrame.from_dict(results_knn)\nresults = results.append(results_knn)\n\nresults\n\n\n\n\n\n\n\n\n\nModel\nTrain Score\nTest Score\nRefit Time\n\n\n\n\n0\nLogistic Regression\n90.5888\n90.6531\n2.2281\n\n\n0\nK-Nearest Neighbors\n92.6191\n89.4877\n0.0682"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#decision-tree-classifier",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#decision-tree-classifier",
    "title": "Bank Marketing Dataset",
    "section": "Decision Tree Classifier",
    "text": "Decision Tree Classifier\n\n#DEcision Tree pipeline\npipe_dt = Pipeline([\n    ('pre',pre),\n    ('dt',DecisionTreeClassifier())\n])\n\n#Model Training and predictions\npipe_dt.fit(X_train,y_train)\ny_pred = pipe_dt.predict(X_test)\ny_pred_train = pipe_dt.predict(X_train)\n\n#Model Evaluation\ntrain_score = accuracy_score(y_train,y_pred_train)\nprint('Train Accuracy:',round(train_score*100,4))\ntest_acc_score = accuracy_score(y_test,y_pred)\nprint('Test Accuracy:',round(test_acc_score*100,4))\ntest_f1_score = f1_score(y_test,y_pred,pos_label='yes',average='binary')\nprint('Test F1 score:',round(test_f1_score*100,4))\n\n#Classification report\nprint(classification_report(y_test,y_pred))\n\n#Confusion Matrix Heatmap\nplt.title('Decision Tree Classifier')\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\");\n\nTrain Accuracy: 100.0\nTest Accuracy: 88.5773\nTest F1 score: 51.1676\n              precision    recall  f1-score   support\n\n          no       0.94      0.93      0.94      7303\n         yes       0.50      0.53      0.51       935\n\n    accuracy                           0.89      8238\n   macro avg       0.72      0.73      0.72      8238\nweighted avg       0.89      0.89      0.89      8238\n\n\n\n\n\n\n\n\n\n\n\n#Gridsearch and refitting\n\nparam_grid = {\n    'dt__criterion':('gini','entropy','logloss'),\n    'dt__splitter':('best','random'),\n#    'dt__class_weight':(None,'balanced'),\n    'dt__max_depth':range(2,15,1)\n    \n}\n\ngrid_dt = GridSearchCV(pipe_dt,param_grid,scoring='accuracy',verbose=1)\nstart = time()\ngrid_dt.fit(X_train, y_train)\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"% (time() - start, len(grid_dt.cv_results_[\"params\"])))\n#best params\nprint('The best params are\\n',grid_dt.best_params_)\n\n#Refit Model Predictions\ny_pred = grid_dt.predict(X_test)\ny_pred_train = grid_dt.predict(X_train)\n\n#Refit Model Evaluation\ntrain_score = accuracy_score(y_train,y_pred_train)\nprint('Train Accuracy:',round(train_score*100,4))\ntest_acc_score = accuracy_score(y_test,y_pred)\nprint('Test Accuracy:',round(test_acc_score*100,4))\ntest_f1_score = f1_score(y_test,y_pred,pos_label='yes',average='binary')\nprint('Test F1 score:',round(test_f1_score*100,4))\n\n#Refit model Classification report\nprint(classification_report(y_test,y_pred))\n\n#Confusion Matrix Heatmap\nplt.title('Decision Tree Classifier')\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\");\n\nFitting 5 folds for each of 78 candidates, totalling 390 fits\nGridSearchCV took 48.39 seconds for 78 candidate parameter settings.\nThe best params are\n {'dt__criterion': 'entropy', 'dt__max_depth': 6, 'dt__splitter': 'best'}\nTrain Accuracy: 91.5812\nTest Accuracy: 91.1508\nTest F1 score: 57.5422\n              precision    recall  f1-score   support\n\n          no       0.94      0.96      0.95      7303\n         yes       0.63      0.53      0.58       935\n\n    accuracy                           0.91      8238\n   macro avg       0.79      0.74      0.76      8238\nweighted avg       0.91      0.91      0.91      8238\n\n\n\n\n\n\n\n\n\n\n\n#appending the results Dataframe\n\nresults_dt = {\n    'Model':['Decision Tree Classifier'],\n    'Train Score':[round(train_score*100,4)],\n    'Test Score':[round(test_acc_score*100,4)],\n    'Refit Time':[round(grid_dt.refit_time_,4)]\n}\nresults_dt = pd.DataFrame.from_dict(results_dt)\nresults = results.append(results_dt)\n\nresults\n\n\n\n\n\n\n\n\n\nModel\nTrain Score\nTest Score\nRefit Time\n\n\n\n\n0\nLogistic Regression\n90.5888\n90.6531\n2.2281\n\n\n0\nK-Nearest Neighbors\n92.6191\n89.4877\n0.0682\n\n\n0\nDecision Tree Classifier\n91.5812\n91.1508\n0.1405"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#svc",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#svc",
    "title": "Bank Marketing Dataset",
    "section": "SVC",
    "text": "SVC\n\n#KNN pipeline\npipe_svc = Pipeline([\n    ('pre',pre),\n    ('svc',SVC(random_state=42))\n])\n\n#Model Training and predictions\npipe_svc.fit(X_train,y_train)\ny_pred = pipe_svc.predict(X_test)\ny_pred_train = pipe_svc.predict(X_train)\n\n#Model Evaluation\ntrain_score = accuracy_score(y_train,y_pred_train)\nprint('Train Accuracy:',round(train_score*100,4))\ntest_acc_score = accuracy_score(y_test,y_pred)\nprint('Test Accuracy:',round(test_acc_score*100,4))\ntest_f1_score = f1_score(y_test,y_pred,pos_label='yes',average='binary')\nprint('Test F1 score:',round(test_f1_score*100,4))\n\n#Classification report\nprint(classification_report(y_test,y_pred))\n\n#Confusion Matrix Heatmap\nplt.title('SVC')\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\");\n\nTrain Accuracy: 91.8665\nTest Accuracy: 90.8837\nTest F1 score: 49.6311\n              precision    recall  f1-score   support\n\n          no       0.93      0.97      0.95      7303\n         yes       0.67      0.40      0.50       935\n\n    accuracy                           0.91      8238\n   macro avg       0.80      0.69      0.72      8238\nweighted avg       0.90      0.91      0.90      8238\n\n\n\n\n\n\n\n\n\n\n\n#Gridsearch and refitting\n\ntuned_params = [\n    {\"svc__kernel\": [\"rbf\"], \"svc__gamma\": [1e-3, 1e-4], \"svc__C\": [1, 10, 100, 1000]},\n    {\"svc__kernel\": [\"linear\"], \"svc__C\": [1, 10]},\n]\n\ngrid_svc = GridSearchCV(pipe_svc,tuned_params,scoring='accuracy',verbose=1)\nstart = time()\ngrid_svc.fit(X_train,y_train)\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"% (time() - start, len(grid_svc.cv_results_[\"params\"])))\n#best params\nprint('The best params are\\n',grid_svc.best_params_)\n\n#Refit Model Predictions\ny_pred = grid_svc.predict(X_test)\ny_pred_train = grid_svc.predict(X_train)\n\n#Refit Model Evaluation\ntrain_score = accuracy_score(y_train,y_pred_train)\nprint('Train Accuracy:',round(train_score*100,4))\ntest_acc_score = accuracy_score(y_test,y_pred)\nprint('Test Accuracy:',round(test_acc_score*100,4))\ntest_f1_score = f1_score(y_test,y_pred,pos_label='yes',average='binary')\nprint('Test F1 score:',round(test_f1_score*100,4))\n\n#Refit model Classification report\nprint(classification_report(y_test,y_pred))\n\n#Confusion Matrix Heatmap\nplt.title('SVC')\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\");\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nGridSearchCV took 812.80 seconds for 10 candidate parameter settings.\nThe best params are\n {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}\nTrain Accuracy: 91.1411\nTest Accuracy: 90.7866\nTest F1 score: 46.3604\n              precision    recall  f1-score   support\n\n          no       0.92      0.98      0.95      7303\n         yes       0.68      0.35      0.46       935\n\n    accuracy                           0.91      8238\n   macro avg       0.80      0.66      0.71      8238\nweighted avg       0.89      0.91      0.89      8238\n\n\n\n\n\n\n\n\n\n\n\n#appending the results Dataframe\n\nresults_svc = {\n    'Model':['SVC'],\n    'Train Score':[round(train_score*100,4)],\n    'Test Score':[round(test_acc_score*100,4)],\n    'Refit Time':[round(grid_svc.refit_time_,4)]\n}\nresults_svc = pd.DataFrame.from_dict(results_svc)\nresults = results.append(results_svc)\nresults.reset_index(drop=True)\n\n\n\n\n\n\n\n\n\nModel\nTrain Score\nTest Score\nRefit Time\n\n\n\n\n0\nLogistic Regression\n90.5888\n90.6531\n2.2281\n\n\n1\nK-Nearest Neighbors\n92.6191\n89.4877\n0.0682\n\n\n2\nDecision Tree Classifier\n91.5812\n91.1508\n0.1405\n\n\n3\nSVC\n91.1411\n90.7866\n30.7087"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#final-results",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#final-results",
    "title": "Bank Marketing Dataset",
    "section": "Final Results",
    "text": "Final Results\n\nresults\n\n\n\n\n\n\n\n\n\nModel\nTrain Score\nTest Score\nRefit Time\n\n\n\n\n0\nLogistic Regression\n90.5888\n90.6531\n2.2281\n\n\n0\nK-Nearest Neighbors\n92.6191\n89.4877\n0.0682\n\n\n0\nDecision Tree Classifier\n91.5812\n91.1508\n0.1405\n\n\n0\nSVC\n91.1411\n90.7866\n30.7087"
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#inferences",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#inferences",
    "title": "Bank Marketing Dataset",
    "section": "Inferences",
    "text": "Inferences\n\nK-Nearest Neighbors classifier took the least amount of time to train but the catch is that it also produced the least accuracy score amongst the other classifiers.\nDecision Tree Classifier produced the highest accuracy predictions while also takin the least amount of time to train losing only to KNN classifier. This can be due to high amount categorical features in the dataset.\nSupport Vector Classifier(SVC) performed decently in predicting the outcome but had a significantly worse training time."
  },
  {
    "objectID": "posts/Bank-Telemarketing/bank-marketing-dataset.html#next-steps-recommendations",
    "href": "posts/Bank-Telemarketing/bank-marketing-dataset.html#next-steps-recommendations",
    "title": "Bank Marketing Dataset",
    "section": "Next Steps & Recommendations",
    "text": "Next Steps & Recommendations\n\nThe focus of this project has been to compare and contrast various classifers, minimal amount of preprocessing has been done to the data before the feeding it into the algorithms. Further cleaning the data by identifying the important features and trying various encoding methods for categorical feartures.\nThe primary metric for evaluation used was “Accuracy”, we can easily change this “Precision” or “Recall” based on the business requirements and redo the experiment.\nTo avoid extremely long runtimes, I have commented out some parameters in the GridSearchCV params grid. The result may vary when more rigorous grid search is peformed."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": ".",
    "section": "",
    "text": "Bank Marketing Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 5, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]